{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Differentiation in Theano vs Sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run below comment to install theano so we can call import theano and whatnot below\n",
    "#! pip install theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhouming (MingMing) Sun\n",
    "\n",
    "## About the method\n",
    "\n",
    "I was really interested by symbolic differentiation, so I tried to do it in a more efficient and cleaner way to understand higher order deriviatives on a function with respect to x. \n",
    "\n",
    "According to wikipedia: \"Theano is a Python library and optimizing compiler for manipulating and evaluating mathematical expressions, especially matrix-valued ones. In Theano, computations are expressed using a NumPy-esque syntax and compiled to run efficiently on either CPU or GPU architectures.\"\n",
    "\n",
    "Theano can do derivatives, but it also has the ability to produce computational graphs, as seen in x_squared_derivative.png and x_cubed_derivative.png. So I thought it would be super neat to have it do a higher order derivative, and then give us a computation graph. My goal is to compare symbolic differentiation results between theano and sympy for basic polynomials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))\n",
      "The derivative is right, once you look past the tensorconstant stuff:   2 * x ^ (2-1)\n",
      "8.0\n",
      "final result below in reduced form, it seems. 2x is indeed the derivative of x^2: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(TensorConstant{2.0} * x)'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import pp\n",
    "\n",
    "#here is a basic example of taking derivative of x^2\n",
    "x = T.dscalar('x')\n",
    "y = x ** 2\n",
    "gy = T.grad(y, x)\n",
    "print(pp(gy))  # print out the gradient prior to optimization\n",
    "print(\"The derivative is right, once you look past the tensorconstant stuff:   2 * x ^ (2-1)\")\n",
    "\n",
    "\n",
    "f = theano.function([x], gy)\n",
    "\n",
    "#calculate value of x=4. 2*4 = 8, seems good\n",
    "print(f(4))\n",
    "\n",
    "print(\"final result below in reduced form, it seems. 2x is indeed the derivative of x^2: \")\n",
    "pp(f.maker.fgraph.outputs[0])\n",
    "#theano.printing.pydotprint(f, outfile=\"looped_derivatives.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((fill((((fill((((fill((((fill(((fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10}) * (x ** (TensorConstant{10} - TensorConstant{1}))), TensorConstant{1.0}) * (fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10})) * (TensorConstant{10} - TensorConstant{1})) * (x ** ((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}))), TensorConstant{1.0}) * ((fill(((fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10}) * (x ** (TensorConstant{10} - TensorConstant{1}))), TensorConstant{1.0}) * (fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10})) * (TensorConstant{10} - TensorConstant{1}))) * ((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1})) * (x ** (((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1}))), TensorConstant{1.0}) * ((fill((((fill(((fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10}) * (x ** (TensorConstant{10} - TensorConstant{1}))), TensorConstant{1.0}) * (fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10})) * (TensorConstant{10} - TensorConstant{1})) * (x ** ((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}))), TensorConstant{1.0}) * ((fill(((fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10}) * (x ** (TensorConstant{10} - TensorConstant{1}))), TensorConstant{1.0}) * (fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10})) * (TensorConstant{10} - TensorConstant{1}))) * ((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}))) * (((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1})) * (x ** ((((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1}))), TensorConstant{1.0}) * ((fill((((fill((((fill(((fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10}) * (x ** (TensorConstant{10} - TensorConstant{1}))), TensorConstant{1.0}) * (fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10})) * (TensorConstant{10} - TensorConstant{1})) * (x ** ((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}))), TensorConstant{1.0}) * ((fill(((fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10}) * (x ** (TensorConstant{10} - TensorConstant{1}))), TensorConstant{1.0}) * (fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10})) * (TensorConstant{10} - TensorConstant{1}))) * ((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1})) * (x ** (((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1}))), TensorConstant{1.0}) * ((fill((((fill(((fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10}) * (x ** (TensorConstant{10} - TensorConstant{1}))), TensorConstant{1.0}) * (fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10})) * (TensorConstant{10} - TensorConstant{1})) * (x ** ((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}))), TensorConstant{1.0}) * ((fill(((fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10}) * (x ** (TensorConstant{10} - TensorConstant{1}))), TensorConstant{1.0}) * (fill((x ** TensorConstant{10}), TensorConstant{1.0}) * TensorConstant{10})) * (TensorConstant{10} - TensorConstant{1}))) * ((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}))) * (((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1}))) * ((((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1})) * (x ** (((((TensorConstant{10} - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1}) - TensorConstant{1})))\n",
      "wow that looks terrible, but it is before optimization\n",
      "\n",
      "967680.0\n",
      "yay that is right. 30240 * 2^5 = 967680 indeed\n",
      "\n",
      "Below is the optimized form. It looks off. IDK what the whole left half is doing. The 30240 is correct though, so it is still somehow right\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Elemwise{Composite{(i0 * Composite{(sqr(sqr(i0)) * i0)}(i1))}}(TensorConstant{30240.0}, x)'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i want to have a function that derives an expression f_x with respect to x, n times. then print out the computational graph\n",
    "\n",
    "#takes in a function of x, x data type, and n number of times to derive f_x\n",
    "def deriv_loop(f_x, x, n):\n",
    "    dx = f_x\n",
    "    for i in range(n):\n",
    "        dx = T.grad(dx, x)\n",
    "    return dx\n",
    "\n",
    "#let's first test on x^10, derived 5 times. The answer should be 30240 * x^5\n",
    "x = T.dscalar('x')\n",
    "y = x ** 10\n",
    "g = deriv_loop(y, x, 5)\n",
    "\n",
    "print(pp(g))\n",
    "print(\"wow that looks terrible, but it is before optimization\")\n",
    "print()\n",
    "\n",
    "h = theano.function([x], g)\n",
    "print(h(2))\n",
    "print(\"yay that is right. 30240 * 2^5 = 967680 indeed\")\n",
    "print()\n",
    "\n",
    "print(\"Below is the optimized form. It looks off. IDK what the whole left half is doing. The 30240 is correct though, so it is still somehow right\")\n",
    "pp(h.maker.fgraph.outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at looped_derivatives.png\n"
     ]
    }
   ],
   "source": [
    "#let's see what the computation graph looks like! \n",
    "theano.printing.pydotprint(h, outfile=\"looped_derivatives.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking out that output file... \n",
    "Well that was not what I was looking for. I was hoping for some complex looking loopy graph, but instead it treated 5 derivatives as one operation. It also returned something weird for the final reduced form of the 5th derivative of x^10. However, the test of h(2) returned the correct answer, so it must be doing something right. (I tested several other values and they all seem correct.)\n",
    "\n",
    "Let's compare this to sympy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{10}$"
      ],
      "text/plain": [
       "x**10"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "from sympy.abc import x\n",
    "k = x ** 10\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 30240 x^{5}$"
      ],
      "text/plain": [
       "30240*x**5"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = sympy.diff(k, x)\n",
    "k = sympy.diff(k, x)\n",
    "k = sympy.diff(k, x)\n",
    "k = sympy.diff(k, x)\n",
    "k = sympy.diff(k, x)\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion about the method\n",
    "\n",
    "Ok, granted, I did hardcode 5 derivatives for sympy, but still. It is a little embarrasing how easy sympy handled this compared to theano. If I were to explore this further, I should compare calculation times, and test more complex functions that aren't just easy polynomials. Perhaps theano will shine there. The theano repeated derivative function was also made by me, and that is probably also a major factor as to why the 5th derivative and computation graph looks like pasta. \n",
    "\n",
    "I guess for the case of very simple polynomial symbolic differentiation, a sympy derivative gives a much clearer answer than a theano handicapped with questionable code. \n",
    "\n",
    "### Questions you have about the method\n",
    "\n",
    "* How exactly does that computation graph work? \n",
    "* Also, what exactly is a tensor? Those seem to be the foundation of theano. \n",
    "\n",
    "## About the software\n",
    "\n",
    "Theano was developed by Montreal Institute for Learning Algorithms (MILA), University of Montreal, up until late 2017\n",
    "\n",
    "Link to the repository: https://github.com/Theano/Theano \n",
    "\n",
    "Official tutorial: http://www.deeplearning.net/software/theano/index.html\n",
    "\n",
    "Here is their summary from their tutorial: \n",
    "\n",
    "Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Theano features:\n",
    "\n",
    "* tight integration with NumPy – Use numpy.ndarray in Theano-compiled functions.\n",
    "* transparent use of a GPU – Perform data-intensive computations much faster than on a CPU.\n",
    "* efficient symbolic differentiation – Theano does your derivatives for functions with one or many inputs.\n",
    "* speed and stability optimizations – Get the right answer for log(1+x) even when x is really tiny.\n",
    "* dynamic C code generation – Evaluate expressions faster.\n",
    "* extensive unit-testing and self-verification – Detect and diagnose many types of errors.\n",
    "* Theano has been powering large-scale computationally intensive scientific investigations since 2007. But it is also approachable enough to be used in the classroom (University of Montreal’s deep learning/machine learning classes).\n",
    "\n",
    "### Open questions\n",
    "\n",
    "* Any open questions you would like to discuss or get help answering?\n",
    "No. See the other questions above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
